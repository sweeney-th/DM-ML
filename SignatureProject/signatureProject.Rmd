---
author: "Thadryan Sweeney"
title: "DMML Signature Assignment"
output: html_notebook
---

```{R Load the data}

# get the data, check the number of rows
data <- read.csv("engineered_tweet_data.csv", header = TRUE, stringsAsFactors = FALSE)
str(data)
```

We'll now divide the dataset up by candidate so we can make a few comparisons. We will also make sure we have a fair proportion of each candidate.

```{R Inspect the data}

# see how many records we lose is we use complete only 
data <- data[complete.cases(data), ]
nrow(data)

# assure we have a reasonable distribution 
prop.table(table(data$handle))

# divide the tweets by candidate for inspection
hc.tweets <- data[which(data$handle == "HillaryClinton"), ]
dt.tweets <- data[which(data$handle == "realDonaldTrump"), ]
```

# Summative statistics and Comparisons

Given that I re-engineered a lot of this dataset, I'm going to start by looking at a few simple side-by-side comparisons to make sure the features are distinguishable between the candidates before trying to use them. 

### Average Sentence Length

```{R Average sentence length}

# z test check normalility
# average word length
mean(hc.tweets$Mean_sentence_length)
mean(dt.tweets$Mean_sentence_length)
```

There is a noticeable difference in the mean sentence length, which is a good sign.

### Average word length

```{R Average }

# effect size
# average word length
mean(hc.tweets$Mean_word_length)
mean(dt.tweets$Mean_word_length)
```

The difference here is likely not going to help us much. Let's keep digging. 

### Retweets

```{R Retweet proportions}

# chi squared test in prop.table
# prop table of retweets
prop.table(table(hc.tweets$is_retweet))
prop.table(table(dt.tweets$is_retweet))
```

It seems that the candidates retweet at noticeably different rates. Good news for our models. 

### Sharing Quotes

```{R Use of quotes?}

# prop table of retweets
prop.table(table(hc.tweets$is_quote_status))
prop.table(table(dt.tweets$is_quote_status))
```

The differences in quotes retweeted seems subtle at best.

### Use of ALLCAPS

```{R Words in caps}

# prop table of words in caps
hc.mean.caps <- mean(hc.tweets$Words_in_caps)
dt.mean.caps <- mean(dt.tweets$Words_in_caps)

# store and plot caps data
caps.counts <- c(hc.mean.caps, dt.mean.caps)
barplot(caps.counts, names = c("Clinton", "Trump"), xlab = "Candidate", ylab = "Words in CAPS", col = "GREEN")
```

### Favorite Counts 

```{R Favorite couts}

hc.meanfav <- mean(as.numeric(hc.tweets$favorite_count))
dt.meanfav <- mean(as.numeric(dt.tweets$favorite_count))

hc.meanfav
dt.meanfav
fav_counts <- c(hc.meanfav, dt.meanfav)
barplot(fav_counts, names = c("Clinton", "Trump"), xlab = "Candidate", ylab = "Av. Favs.", col = "BLUE")
```

The difference here is extremely noticeable.

After inspecting a handful of differences, I'm confident that the combination of previously existing and engineered features will have enough differences for a model to sort them.

We will now make a copy of the dataset to convert to numeric and factors so we can apply models to it.

```{R Create numeric dataset}

num.data <- data

# the text is no longer needed
num.data$text <- NULL

# binary dummy codes for Clinton and Trump
num.data$handle[which(num.data$handle == "HillaryClinton") ] <- 1
num.data$handle[which(num.data$handle == "realDonaldTrump") ] <- 0

# simple binary codes for booleans 
num.data$is_retweet[which((num.data$is_retweet) == "True")] <- 1
num.data$is_retweet[which((num.data$is_retweet) == "False")] <- 0

num.data$is_quote_status[which((num.data$is_quote_status) == "True")] <- 1
num.data$is_quote_status[which((num.data$is_quote_status) == "False")] <- 0

num.data$truncated[which((num.data$truncated) == "True")] <- 1
num.data$truncated[which((num.data$truncated) == "False")] <- 0

# numeric/factor data associated to create an ID-like structure
# for example this will assign each language a number, each original 
# author a number, and each "in reply" unique number
num.data$lang <- as.numeric(factor(num.data$lang))
num.data$original_author <- as.numeric(factor(num.data$original_author))
num.data$in_reply_to_screen_name <- as.numeric(factor(num.data$in_reply_to_screen_name))

# iterate through the dataset and convert to numeric
for (i in 1:ncol(num.data) )
{
  num.data[,i] <- as.numeric(num.data[,i])
}

# inspec the dataset to verify it worked the way we want it to
str(num.data)
```

### Partition the Datset

Now that we have seen their are patterns in the data for models to identify and converted our numbers to 

```{R Partition Data}

library(caret)

# set seed 
set.seed(867.5309)

# create index for partitioning 
trainIndex <- createDataPartition(num.data$handle, p = 0.5, list = FALSE)

# entries in the index are train, the ones that aren't are test
train <- num.data[trainIndex, ]
train <- train[-3053, ]   # we have an odd numer of samples, some models reject this

test <- num.data[-trainIndex, ]
```


Now that we are equipped with a numeric dataset, we can start applying some preliminary models to see what strategies might get good results for our dataset.

# Linear Regression for Prediciton and Exploration

Firstly, we will try a linear regression model to see if we can get good predictions and hints as to which features might be the most valuable 

```{R linear exploration}

# we will be using the rmse() function throughout this analysis
library(Metrics)

# establish a linear model 
lin <- lm(handle ~., data = train)

# use the model to make predictions 
test$lin.pred <- round(predict(lin, data = test))

# evaluate performace by simple accuracy and rmse
length(which(test$lin.pred == test$handle))/nrow(test) # simple accuracy
rmse(test$lin.pred, test$handle) # RMSE
```

These are quite underwhelming; it doesn't bode well for a model if the simple accuracy can be confused for its RMSE. But the reason I included it is because a summary of the model will give us a good idea of which variable are statistically significant. We could, of course, try a few different techniques to improve the regression model, but given the lackluster performance we'll try some other methods first as I suspect some will be more promising out-of-the-box and we can revisit it if needed. In practice, the p-values given in the summary were of as much interest to me as the results.

```{R P-Values in the LR model}

# summary of the LRM
summary(lin)
test$lin.pred <- NULL
```

This readout provides an excellent readout of features useful in the prediction of the author of the tweet. As noted in the key, the "." and "*" characters dictate the significance to the model. It should be noted that these were considered significant to a model that did not result in useful predictions in practice, but it is useful as a place to start. 

### The k-NN Model

The k-NN model is a simple,venerated approach to numeric data based on euclidean distance, and so makes a good starting point for us. It doesn't technically produce a model; it simply compares new data to periodical observations it has for reference, but can often compete with more sophisticated models and is worth a try for numerical data classification tasks.

```{R the k-NN}

# this will require factored outcomes
train$handle <- as.factor(train$handle)
test$handle <- as.factor(test$handle)

# create a caret based knn model
knn <- train(handle ~., data = train, method = "knn")
knn
```

We can now look at how it will work in context.

```{R Evaluate k-NN}

# apply the model to the testing set
knn.pred <- predict(knn, test)

# see how it did
length(which(knn.pred == test$handle))/nrow(test)
rmse(as.numeric(knn.pred), as.numeric(test$handle))
```

This is a fairly good start, so we will invest the time to inspect it further and tune it. We will use the trainControl() function to manipulate tuning parameters.

```{R Tuning k-NN}

# define basic parameters
ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "best")

tuned.knn <- train(handle ~., data = test, method = "knn", trControl = ctrl)
tuned.knn
```

Let's evaluate the performance of out tuned k-NN model.

```{R Evaluating the tuned k-NN}

# apply the model to the testing set
tuned.knn.pred <- predict(tuned.knn, test)

# see how it did
length(which(tuned.knn.pred == test$handle))/nrow(test)
rmse(as.numeric(tuned.knn.pred), as.numeric(test$handle))
```

Our tuning pays of with a nudge in the right direction for both metrics. We have some compelling evidence that we are on the right track, so let's take a look at a more detailed evaluation. We will use the ROCR package to measure the area under the curve of our models performance charted by true positives and negative. This entails created and object for the model to get some information from at then plotting it.

```{R k-NN AUC}

library(ROCR)

# create an ROC object to get the ROC
knn.pred.obj <- prediction(predictions = as.numeric(tuned.knn.pred), labels = factor(test$handle))

# evaluate the performace of the object "true positive rates" and "false positive rates"
perf <- performance(knn.pred.obj, measure = "tpr", x.measure = "fpr")

# plot the results 
plot(perf, main = "ROC curve for knn model", col = "green", lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2)
```

The further away from the midline our model gets, the better it is. The visual is helpful and suggests we are doing pretty well, but we can quantify for more accurate comparisons. This is the numeric output of the area under the curve.

```{R Quantify k-NN AUC}

# quantify results
knn.perf.metrics <- performance(knn.pred.obj, measure = "auc")
unlist(knn.perf.metrics@y.values)
```

The tuned k-NN easily outperformed our mostly-exploratory linear model, and readily benefited from a little tuning, so we will keep that as one of our three models for now. 

# Nueral Network 

Next, we will try a neural network. Neural nets operate as a layer of functions that can activate or remain dormant based on their input, and in turn relay information to another layer of functions. The parameters that we can manipulate are the number of layers and the thresholds for activation. Training the neural net produces a vast amount of output so, while this will run, I've opted to conceal the text output with the invisible() function.

```{R Train Neural Network}

# train a nueral net
nnet <- invisible(train(handle ~., data = train, method = "nnet"))
```

We can use our network to make predictions.

```{R Neural Network Predictions}

# make predictions based on our test data
nn.pred <- predict(nnet, test)

# evaluate with out metrics
length(which(nn.pred == test$handle))/nrow(test)
rmse(as.numeric(nn.pred), as.numeric(test$handle))
```

This is our best result so far, but let's see if we can coax anything else out of it.

```{R Tuning the Neural Net: Train}

# establish control settings for the models 
# some testing revealed that cv worked better for this model than rpeatedCV
ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "best")

# created nn model 
tuned.nnet <- invisible(train(handle ~., data = test, method = "nnet", trControl = ctrl))
```

We can now evaluate the new model. 

```{R Tuning th Nueral Net: Evaluate}

# make predictions
tuned.nn.pred <- predict(tuned.nnet, test)

# evaluate with our metrics
length(which(tuned.nn.pred == test$handle))/nrow(test)
rmse(as.numeric(tuned.nn.pred), as.numeric(test$handle))
```

Again, we see improvements based on a little bit of tuning. To we'll evaluate the predictive power in the same way we did form the k-NN model. 

```{R Neural Net AUC}

# create an ROC object to get the ROC
tuned.nn.pred.obj <- prediction(predictions = as.numeric(tuned.nn.pred), labels = factor(test$handle))

# evaluate the performace of the object "true positive rates" and "false positive rates"
tuned.nn.perf <- performance(tuned.nn.pred.obj, measure = "tpr", x.measure = "fpr")

# plot the results 
plot(tuned.nn.perf, main = "ROC curve for Neural Net model", col = "gray", lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2)
```

Finally, we see the AUC metric for the tuned neural network.

```{R Quantify Nueral Net AUC}

# quantify results 
tuned.nn.perf.metrics <- performance(tuned.nn.pred.obj, measure = "auc")
unlist(tuned.nn.perf.metrics@y.values)
```

# The Random Forest Model

```{R Random Forest}

ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "best")
tuned.rf <- train(handle ~., data = test, method = "rf", trControl = ctrl)

# apply the model to the testing set
tuned.rf.pred <- predict(tuned.rf, test)

# see how it did
length(which(tuned.rf.pred == test$handle))/nrow(test)
rmse(as.numeric(tuned.rf.pred), as.numeric(test$handle))

# create an ROC object to get the ROC
tuned.rf.pred.obj <- prediction(predictions = as.numeric(tuned.rf.pred), labels = factor(test$handle))

# evaluate the performace of the object "true positive rates" and "false positive rates"
tuned.rf.perf <- performance(tuned.rf.pred.obj, measure = "tpr", x.measure = "fpr")

# plot the results 
plot(tuned.rf.perf, main = "ROC curve for X model", col = "gray", lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2)
tuned.rf.perf.metrics <- performance(tuned.rf.pred.obj, measure = "auc")
unlist(tuned.rf.perf.metrics@y.values)
```
 ### (re)Evaluating the Dubiously Accurate RF Model
 
This alarming rate of success is not necessarily a good sign. It could be that we have overfit the model to the point where it will not work for datasets other than this one. That is not a disaster in this context, but can foil a lot of hard work in environments where the models are intended to scale and be applied to new datasets on a regular basis. To see if the model will work on new data, we will create a new, randomized, partition from the dataset and run it again. This will simulate, to some extent, running the model on an unseen dataset. 

```{R Re-evluate Random Forest}

# we will take another random 50% sample from the dataset and run the RF model on it
trainIndex2 <- createDataPartition(num.data$handle, p = 0.5, list = FALSE)

# create set based on partition index
test2 <- num.data[trainIndex2, ]

# factor the new data set 
test2$handle <- as.factor(test2$handle)

# use it for predictions
tuned.rf.pred2 <- predict(tuned.rf, test2)

# evaluate performance
length(which(tuned.rf.pred2 == test2$handle))/nrow(test2)
rmse(as.numeric(tuned.rf.pred2), as.numeric(test2$handle))
```

```{R RF "New" Data - AUC}

# create an ROC object to get the ROC
tuned.rf.pred.obj2 <- prediction(predictions = as.numeric(tuned.rf.pred2), labels = factor(test2$handle))

# evaluate the performace of the object "true positive rates" and "false positive rates"
tuned.rf.perf2 <- performance(tuned.rf.pred.obj2, measure = "tpr", x.measure = "fpr")

# plot the results 
plot(tuned.rf.perf2, main = "ROC curve for Neural Net model", col = "gray", lwd = 3)
abline(a = 0, b = 1, lwd = 2, lty = 2)
```

```{R RF "New" Data - AUC metric}
# quantify
tuned.rf.perf.metrics2 <- performance(tuned.rf.pred.obj2, measure = "auc")
unlist(tuned.rf.perf.metrics2@y.values)
```

Given that we don't have an entirely new dataset for me to try this on, using the same model on a newly randomized test set from the original and still performed strongly will have to justify that it has a place in our ensemble.

# Avengers Assemble: Creating and Ensemble Model 

In practice, most profesional data scientists will create ensemble models where different models are given a vote and the final answer is the consensus between them. This can help shore up weaknesses in individual models and prevent overfitting. Sometimes, especially acurate models are given more than one vote, which we will do for our RF model. 

k-NN, RF, RF, NN, 

```{R Ensemble}

test$knn.pred <- as.numeric(factor(predict(tuned.knn, test)))
test$nnet <- as.numeric(factor(predict(tuned.nnet, test)))
test$rf.pred <- as.numeric(factor(predict(tuned.rf, test)))
test$rf.pred.v2 <- test$rf.pred


#test$vote <- names(which.max(table((c(test$knn.pred, test$nnet, test$rf.pred,
#                                              test$rf.pred, test$rf.pred)))))
#test$vote <- as.factor(factor(test$vote))

length(which(test$vote == test$handle))/nrow(test)
```

























# Support Vector Machine Model 

Next we will try an SVM model. We already have our train/test partitions and our dataset is primed for numeric analysis, so we can dig right in after importing the appropriate libraries.

```{R SVM model, eval=FALSE, include=FALSE}

# import library
library(kernlab)

# remove the old prediction 
lin.pred <- NULL

# create SVM predicting handle based on partitions 
svm <- ksvm(handle ~., data = train, kernel = "vanilladot") # the bug is from here!

# use it to make predictions
test$svm.raw.pred <- predict(svm, test)
test$svm.pred <-test$svm.raw.pred

### examine the performance 
# simple accuracy
length(which(test$svm.pred == test$handle))/nrow(test)

# rmse
rmse(test$svm.pred, test$handle)
```

With an accuracy of just over 80% and a sub 0.5 RMSE on the first pass, this model shows much more potential. Given these findings, we'll spend a little more time evaluating the model to see if we want to invest the time to tune it. 

### AUC for the SVM model

```{R More detailed investigation of the SVM model, eval=FALSE, include=FALSE}

# get libraries for AUC analysis
library(ROCR)

# create an ROC object to get the ROC
svm.pred.obj <- prediction(predictions = as.numeric(test$svm.pred), labels = factor(test$handle))

# evaluate the performace of the object "true positive rates" and "false positive rates"
perf <- performance(svm.pred.obj, measure = "tpr", x.measure = "fpr")

# plot the results 
plot(perf, main = "ROC curve for SVM model", col = "blue", lwd = 3)
abline(a = 0, b = 1, lwd =2, lty = 2)
```

This is a helpful visual, but is best complemented with a quantitative representation.

```{R SVM performance 2, eval=FALSE, include=FALSE}

# create object for AUC results 
svm.perf.metrics <- performance(svm.pred.obj, measure = "auc")
unlist(svm.perf.metrics@y.values)
```

Based on typical interpretation of this figure, our model just squeaks into the "Excellent/good" range (this figure approximates letter grades and is presented as a heuristic in our text book).

We now have three metrics of varying levels of sophistication putting the SVM model somewhere between fair and excellent, so we'll officially use this as one of our three for the assignment. It is possible we can still squeeze some performance out of this approach before moving on however.

### Tuning the SVM

```{R Tuning the SVM, eval=FALSE, include=FALSE}

train$handle <- as.factor(train$handle)
test$handle <- as.factor(test$handle)

tuned.svm <- train(handle ~., data = train, method = "svmRadial")
```
