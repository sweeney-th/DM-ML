---
author: "Thadryan Sweeney"
title: "DMML Signature Assignment"
output: html_notebook
---

```{R Load the data}

# get the data, check the number of rows
data <- read.csv("engineered_tweet_data.csv", header = TRUE, stringsAsFactors = FALSE)
str(data)
```


```{R Inspect the data}

# see how many records we lose is we use complete only 
data <- data[complete.cases(data), ]
nrow(data)

# assure we have a reasonable distribution 
prop.table(table(data$handle))

# divide the tweets by candidate for inspection
hc.tweets <- data[which(data$handle == "HillaryClinton"), ]
dt.tweets <- data[which(data$handle == "realDonaldTrump"), ]
```

# Summative statistics 

### Average sentence length

```{R Average sentence length}

# average word length
mean(hc.tweets$Mean_sentence_length)
mean(dt.tweets$Mean_sentence_length)
```

### Average word length

```{R Average }

# average word length
mean(hc.tweets$Mean_word_length)
mean(dt.tweets$Mean_word_length)
```

```{R Retweet proportions}

# prop table of retweets
prop.table(table(hc.tweets$is_retweet))
prop.table(table(dt.tweets$is_retweet))
```



```{R Use of quotes?}

# prop table of retweets
prop.table(table(hc.tweets$is_quote_status))
prop.table(table(dt.tweets$is_quote_status))
```

```{R Favorite couts}

hc.meanfav <- mean(as.numeric(hc.tweets$favorite_count))
dt.meanfav <- mean(as.numeric(dt.tweets$favorite_count))

fav_counts <- c(hc.meanfav, dt.meanfav)
barplot(fav_counts, names = c("Clinton", "Trump"), xlab = "Candidate", ylab = "Av. Favs.", 
                                                                             col = "BLUE")
```

```{R Create numeric dataset}

num.data <- data

num.data$text <- NULL

num.data$handle[which(num.data$handle == "HillaryClinton") ] <- 1
num.data$handle[which(num.data$handle == "realDonaldTrump") ] <- 0

num.data$is_retweet[which((num.data$is_retweet) == "True")] <- 1
num.data$is_retweet[which((num.data$is_retweet) == "False")] <- 0

num.data$is_quote_status[which((num.data$is_quote_status) == "True")] <- 1
num.data$is_quote_status[which((num.data$is_quote_status) == "False")] <- 0

num.data$truncated[which((num.data$truncated) == "True")] <- 1
num.data$truncated[which((num.data$truncated) == "False")] <- 0

num.data$lang <- as.numeric(factor(num.data$lang))
num.data$original_author <- as.numeric(factor(num.data$original_author))
num.data$in_reply_to_screen_name <- as.numeric(factor(num.data$in_reply_to_screen_name))

for (i in 1:ncol(num.data) )
{
  num.data[,i] <- as.numeric(num.data[,i])
}
str(num.data)
```


```{R Partition Data}

library(caret)

# set seed 
set.seed(867.5309)

# create index for partitioning 
trainIndex <- createDataPartition(num.data$handle, p = 0.5, list = FALSE)

# entries in the index are train, the ones that aren't are test
train <- num.data[trainIndex, ]
train <- train[-3053, ]   # we have an odd numer of samples, some models reject this

test <- num.data[-trainIndex, ]



```


Now that we are equipped with a numeric dataset, we can start applying some preliminary models to see what strategies might get good results for our dataset.

# Linear Regression for Prediciton and Exploration

Firstly, we will try a linear regression model to see if we can get good predictions and hints as to which features might be the most valuable 

```{R linear exploration}

# we will be using the rmse() function throughout this analysis
library(Metrics)

# establish a linear model 
lin <- lm(handle ~., data = train)

# use the model to make predictions 
test$lin.pred <- round(predict(lin, data = test))

# evaluate performace by simple accuracy and rmse
length(which(test$lin.pred == test$handle))/nrow(test) # simple accuracy
rmse(test$lin.pred, test$handle) # RMSE
```

These are quite underwhelming; it doesn't bode well for a model if the simple accuracy can be confused for its RMSE. But the reason I included it is because a summary of the model will give us a good idea of which variable are statistically significant. We could, of course, try a few different techniques to improve the regression model, but given the lackluster performance we'll try some other methods first as I suspect some will be more promising out-of-the-box and we can revisit it if needed. In practice, the p-values given in the summary were of as much interest to me as the results.

```{R P-Values in the LR model}

# summary of the LRM
summary(lin)
```

This readout provides an excellent readout of features useful in the prediction of the author of the tweet. As noted in the key, the "." and "*" characters dictate the significance to the model. It should be noted that these were considered significant to a model that did not result in useful predictions in practice, but it is useful as a place to start. 

# Support Vector Machine Model 

Next we will try an SVM model. We already have our train/test partitions and our dataset is primed for numeric analysis, so we can dig right in after importing the appropriate libraries.

```{R SVM model}

# import library
library(kernlab)

# remove the old prediction 
lin.pred <- NULL

# create SVM predicting handle based on partitions 
svm <- ksvm(handle ~., data = train, kernel = "rbfdot") # the bug is from here!

# use it to make predictions
test$svm.raw.pred <- predict(svm, test)
test$svm.pred <- round(test$svm.raw.pred, digits = 0)

### examine the performance 
# simple accuracy
length(which(test$svm.pred == test$handle))/nrow(test)

# rmse
rmse(test$svm.pred, test$handle)
```

With an accuracy of just over 80% and a sub 0.5 RMSE on the first pass, this model shows much more potential. Given these findings, we'll spend a little more time evaluating the model to see if we want to invest the time to tune it. 

### AUC for the SVM model

```{R More detailed investigation of the SVM model}

# get libraries for AUC analysis
library(ROCR)

# create an ROC object to get the ROC
svm.pred.obj <- prediction(predictions = as.numeric(test$svm.pred), labels = factor(test$handle))

# evaluate the performace of the object "true positive rates" and "false positive rates"
perf <- performance(svm.pred.obj, measure = "tpr", x.measure = "fpr")

# plot the results 
plot(perf, main = "ROC curve for SVM model", col = "blue", lwd = 3)
abline(a = 0, b = 1, lwd =2, lty = 2)
```

This is a helpful visual, but is best complemented with a quantitative representation.

```{R SVM performance 2}

# create object for AUC results 
svm.perf.metrics <- performance(svm.pred.obj, measure = "auc")
unlist(svm.perf.metrics@y.values)
```

Based on typical interpretation of this figure, our model just squeaks into the "Excellent/good" range (this figure approximates letter grades and is presented as a heuristic in our text book).

We now have three metrics of varrying levels of sophistication putting the SVM model somewhere between fair and excellent, so we'll officially use this as one of our three for the assignment. It is possible we can still squeeze some performace out of this approach before moving on however.

### Tuning the SVM

```{R Tuning the SVM, eval=FALSE, include=FALSE}

train$handle <- as.factor(train$handle)
test$handle <- as.factor(test$handle)

tuned.svm <- train(handle ~., data = train, method = "svmRadial")
```

### The k-NN Model

The k-NN model is a venerated approach based on euclidian distance. It doesn't technically produce a model; it simply compares new data to eperical oservations it has for refference, but can often compete with more sophisticated models and is worth a try for numerical data classification tasks.

```{R the k-NN}

# this will require factored outcomes
train$handle <- as.factor(train$handle)
test$handle <- as.factor(test$handle)

# create a caret based knn model
knn <- train(handle ~., data = train, method = "knn")
knn
```

We can now look at how it will work in context

```{R Evaluate k-NN}

# apply the model to the testing set
knn.pred <- predict(knn, test)

# see how it did
length(which(knn.pred == test$handle))/nrow(test)
rmse(as.numeric(knn.pred), as.numeric(test$handle))
```

This is a fairly good start. It's basically on par with the SVM already, so it is certainly worth considering and taking the time to tune.
```{R Tuning k-NN}

ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
tuned.knn <- train(handle ~., data = test, method = "knn", trControl = ctrl)
tuned.knn
```
```{R Evaluating the tuned k-NN}

# apply the model to the testing set
tuned.knn.pred <- predict(tuned.knn, test)

# see how it did
length(which(tuned.knn.pred == test$handle))/nrow(test)
rmse(as.numeric(tuned.knn.pred), as.numeric(test$handle))
```

We can use the same approach to further our investigation of our k-NN model.

```{R k-NN AUC}

# create an ROC object to get the ROC
knn.pred.obj <- prediction(predictions = as.numeric(tuned.knn.pred), labels = factor(test$handle))

# evaluate the performace of the object "true positive rates" and "false positive rates"
perf <- performance(knn.pred.obj, measure = "tpr", x.measure = "fpr")

# plot the results 
plot(perf, main = "ROC curve for knn model", col = "green", lwd = 3)
abline(a = 0, b = 1, lwd =2, lty = 2)
```

And the number:

```{R Quantify k-NN AUC}

# quantify results
knn.perf.metrics <- performance(knn.pred.obj, measure = "auc")
unlist(knn.perf.metrics@y.values)
```


```{R Neural Network}

# train a nueral net
nnet <- train(handle ~., data = train, method = "nnet")

# make predictions
nn.pred <- predict(nnet, test)

# evaluate with out metrics
length(which(nn.pred == test$handle))/nrow(test)
rmse(as.numeric(nn.pred), as.numeric(test$handle))
```

This is our best result so far, so let's see if we can coax anything else out of it.

```{R Tuning the Neural Net}

ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
tuned.nnet <- train(handle ~., data = test, method = "nnet", trControl = ctrl)

# make predictions
tuned.nn.pred <- predict(tuned.nnet, test)

# evaluate with out metrics
length(which(tuned.nn.pred == test$handle))/nrow(test)
rmse(as.numeric(tuned.nn.pred), as.numeric(test$handle))
```


```{R Neural Net AUC}

# create an ROC object to get the ROC
tuned.nn.pred.obj <- prediction(predictions = as.numeric(tuned.nn.pred),
                                            labels = factor(test$handle))

# evaluate the performace of the object "true positive rates" and "false positive rates"
perf <- performance(tuned.nn.pred.obj, measure = "tpr", x.measure = "fpr")

# plot the results 
plot(perf, main = "ROC curve for Neural Net model", col = "gray", lwd = 3)
abline(a = 0, b = 1, lwd =2, lty = 2)
```

```{R Quantify Nueral Net AUC}

# quantify results
tuned.nn.perf.metrics <- performance(tuned.nn.pred.obj, measure = "auc")
unlist(tuned.nn.perf.metrics@y.values)
```